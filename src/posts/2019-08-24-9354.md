---
title: "Post-processing Large CSV Files Using the CLI"
date: 2019-08-24
categories:
  - notes
authors:
  - chris
archived: true
---

I generated 1.1TB of string data for [a project](/blog/9331/), overnight. It's just one big text file on a disk. Now I just have to grep through it to find the particular patterns I need… that 1.1TB will probably come down to 500-600GB by the end of it, but I can see the pattern-matching process taking the rest of the weekend…

Python and command-line utilities have been super useful at generating this data, and definitely helped the process along. As a reminder to myself, these are the commands I'm using to "post-process" the data:

Look for lines in input.csv which **don't** match this pattern, and echo them to output.csv:

```bash
$ grep -vE "([A-K]{3}),\1" input.csv > output.csv
```

Split output.csv into files 800MB in size, called data\__n_, where _n_ is an 8-digit incremental number (e.g. data_00000001):

```bash
$ split -a 8 -d -b 800M output.csv data_
```

For each data file in the directory, give it the .csv extension:

```bash
$ for f in data*; mv "$f" "$f.csv"; done
```
